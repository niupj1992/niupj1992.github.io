<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="ceph,architecture," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="Ceph架构  本文主要翻译自官网，原文：http://ceph.com/docs/master/architecture/ Ceph在同一平台提供了统一的对象， 块设备，和文件存储。Ceph高度可靠，容易管理，以及免费。使用Ceph作为存储架构，能够使得公司的互联网基础设施架构和公司员工个人的才能得到很大的改造和提升，当然也表现在易于实现大量数据管理方面。Ceph提供额外的扩展性–成千的客户端同">
<meta name="keywords" content="ceph,architecture">
<meta property="og:type" content="article">
<meta property="og:title" content="ceph 架构">
<meta property="og:url" content="http://yoursite.com/2017/07/24/ceph-architecture/index.html">
<meta property="og:site_name" content="niupj&#39;s blog">
<meta property="og:description" content="Ceph架构  本文主要翻译自官网，原文：http://ceph.com/docs/master/architecture/ Ceph在同一平台提供了统一的对象， 块设备，和文件存储。Ceph高度可靠，容易管理，以及免费。使用Ceph作为存储架构，能够使得公司的互联网基础设施架构和公司员工个人的才能得到很大的改造和提升，当然也表现在易于实现大量数据管理方面。Ceph提供额外的扩展性–成千的客户端同">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://i.imgur.com/AnaqOwN.png">
<meta property="og:image" content="http://i.imgur.com/WvZYpEr.png">
<meta property="og:image" content="http://i.imgur.com/EYYzL6y.png">
<meta property="og:image" content="http://i.imgur.com/7wl8ust.png">
<meta property="og:image" content="http://i.imgur.com/3ZCiwLl.png">
<meta property="og:image" content="http://i.imgur.com/95Y1ToP.png">
<meta property="og:image" content="http://i.imgur.com/m1DgSjd.png">
<meta property="og:image" content="http://i.imgur.com/NN1QUZa.png">
<meta property="og:image" content="http://i.imgur.com/y18VDFp.png">
<meta property="og:image" content="http://i.imgur.com/Z1V046i.png">
<meta property="og:image" content="http://i.imgur.com/ZqVIh3I.png">
<meta property="og:image" content="http://i.imgur.com/iqPVUcU.png">
<meta property="og:image" content="http://i.imgur.com/rELGQFi.png">
<meta property="og:image" content="http://i.imgur.com/JSahM49.png">
<meta property="og:image" content="http://i.imgur.com/VJ4pAji.png">
<meta property="og:image" content="http://i.imgur.com/tsjzfx5.png">
<meta property="og:image" content="http://i.imgur.com/nXpimbX.png">
<meta property="og:image" content="http://i.imgur.com/lUIZqYg.png">
<meta property="og:image" content="http://i.imgur.com/4sDFCUE.png">
<meta property="og:image" content="http://i.imgur.com/df9JTKb.png">
<meta property="og:image" content="http://i.imgur.com/Znlg4s0.png">
<meta property="og:image" content="http://i.imgur.com/3iCMLYi.png">
<meta property="og:image" content="http://i.imgur.com/Jl6ePPg.png">
<meta property="og:image" content="http://i.imgur.com/WwCMDZX.png">
<meta property="og:image" content="http://i.imgur.com/ZJqRvHm.png">
<meta property="og:image" content="http://i.imgur.com/E7wYa5U.png">
<meta property="og:updated_time" content="2017-08-20T14:10:50.945Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ceph 架构">
<meta name="twitter:description" content="Ceph架构  本文主要翻译自官网，原文：http://ceph.com/docs/master/architecture/ Ceph在同一平台提供了统一的对象， 块设备，和文件存储。Ceph高度可靠，容易管理，以及免费。使用Ceph作为存储架构，能够使得公司的互联网基础设施架构和公司员工个人的才能得到很大的改造和提升，当然也表现在易于实现大量数据管理方面。Ceph提供额外的扩展性–成千的客户端同">
<meta name="twitter:image" content="http://i.imgur.com/AnaqOwN.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"hide","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/07/24/ceph-architecture/"/>





  <title>ceph 架构 | niupj's blog</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">niupj's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/24/ceph-architecture/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="niupj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="niupj's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">ceph 架构</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-24T22:24:12+08:00">
                2017-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ceph/" itemprop="url" rel="index">
                    <span itemprop="name">ceph</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Ceph架构 </p>
<p>本文主要翻译自官网，原文：<a href="http://ceph.com/docs/master/architecture/" target="_blank" rel="external">http://ceph.com/docs/master/architecture/</a></p>
<p><a href="http://docs.ceph.com/docs/master/glossary/#term-ceph" target="_blank" rel="external">Ceph</a>在同一平台提供了统一的<br><strong>对象</strong>， <strong>块设备</strong>，和<strong>文件存储</strong>。Ceph高度可靠，容易管理，以及免费。使用Ceph作为存储架构，能够使得公司的互联网基础设施架构和公司员工个人的才能得到很大的改造和提升，当然也表现在易于实现大量数据管理方面。Ceph提供额外的扩展性–成千的客户端同时访问PB和EB数量级的数据。一个Ceph节点包括存储硬件设备以及智能的守护进程，而一个ceph存储集群又包含着大量的节点，Ceph节点之间相互通讯，联系以实现动态的数据副本拷贝和数据重新分配。</p>
<a id="more"></a>
<p><img src="http://i.imgur.com/AnaqOwN.png" alt=""></p>
<h1 id="Ceph存储集群"><a href="#Ceph存储集群" class="headerlink" title="Ceph存储集群"></a>Ceph存储集群</h1><p>Ceph基于RADOS(Reliable Autonomic Distributed Object Store)提供了极易扩展的存储集群，详细可以阅读<a href="https://ceph.com/wp-content/uploads/2016/08/weil-rados-pdsw07.pdf" target="_blank" rel="external">论文</a></p>
<p>一个Ceph存储集群包含两类守护进程</p>
<ul>
<li><p>Ceph Monitor </p>
</li>
<li><p>Ceph OSD Daemon</p>
</li>
</ul>
<p><img src="http://i.imgur.com/WvZYpEr.png" alt=""></p>
<p>Ceph monitor维护了集群map的主副本。Ceph的monitors集群通过监控守护进程的失效确保了集群的高可靠性。存储集群客户端通过montior检索集群map的副本。</p>
<p>Ceph OSD 守护进程检查自身的运行状态以及其他OSDs的状态并报告给monitors。</p>
<p>存储集群客户端和每一个OSD守护进程使用CRUSH算法来高效的计算数据存储位置，而不是依赖一个中心查询表。较之于其它的存储架构，Ceph的显著特点在于，通过librados为Ceph存储提供了本地的存储实现接口，并且基于librados可定制许多应用服务接口。</p>
<h2 id="存储数据"><a href="#存储数据" class="headerlink" title="存储数据"></a>存储数据</h2><p> Ceph存储集群数据来自于Ceph客户端–无论是通过Ceph块设备存储、Ceph对象存储、Ceph文件存储或者是通过 librados 定制的实现，存储的数据都是以objects的形式存在的。每个object对应一个在文件系统上显示的file，存储在OSD中。Ceph OSD守护进程处理对存储磁盘的读/写操作。 </p>
<p><img src="http://i.imgur.com/EYYzL6y.png" alt=""></p>
<p>Ceph OSD 守护进程将所有的数据按照objects的存储形式存储在一个扁平化的命名空间内（例如：一个没有层级关系的目录）。一个object包括id，二进制数据以及元数据（包括一个多对name/vaule的集合）。语义（即：命名解析的格式）由Ceph客户端决定。例如，CephFS利用元数据存储文件的属性如，文件所有者，创建时间，最后修改的日期等。<br><img src="http://i.imgur.com/7wl8ust.png" alt=""></p>
<blockquote>
<p>注: 对象的ID在整个集群中都是唯一的，并不只是本地文件系统。</p>
</blockquote>
<h2 id="可扩展和高可用性"><a href="#可扩展和高可用性" class="headerlink" title="可扩展和高可用性"></a>可扩展和高可用性</h2><p> &#160; &#160; &#160; &#160;在传统架构中，客户端需要和中心组件交互（例如网关，broker，API，facade等），中心组件往往是进入一个复杂子系统的唯一入口。这就必然使得集群的性能和可靠性受到限制，导致了单点故障的产生（例如：中心组件出现故障，可能使得整个系统都崩溃）。</p>
<p>&#160; &#160; &#160; &#160;Ceph去除了中心网关来使客户端能够和OSD守护进程直接交互。OSD守护进程在其他节点上创建对象副本来确保数据安全和高可用性。Ceph同时使用了一个monitor集群来确保可靠性。为了实现去中心化，Ceph使用了CRUSH算法。</p>
<h3 id="CRUSH介绍"><a href="#CRUSH介绍" class="headerlink" title="CRUSH介绍"></a>CRUSH介绍</h3><p>&#160; &#160; &#160; &#160;Ceph客户端和OSD守护进程使用CRUSH算法来实现高效的计算出对象存储位置，而不是依赖于一个中心的查询表。CRUSH提供了一个更好的数据管理机制，能够大规模比率的给在集群中的所有客户端和OSD守护进程分配计算任务。接下来的章节提供了CRUSH如何运作的额外细节。CRUSH的详细讨论见<a href="https://ceph.com/wp-content/uploads/2016/08/weil-crush-sc06.pdf" title="论文" target="_blank" rel="external">论文</a>.</p>
<h3 id="集群map"><a href="#集群map" class="headerlink" title="集群map"></a>集群map</h3><p>Ceph需要客户端和OSD守护进程知道集群的拓扑，包括了5个映射集合，这就是 “集群map”。</p>
<p>1.The Monitor Map: 包括集群的 <code>fsid</code> ，位置，命名地址，和每一个monitor端口。同时显示了当前版本号，创建时间，和最后的修改时间。查看monitor的map，通过命令 <code>ceph mon dump</code> 。</p>
<p>2.The OSD Map: 包括集群 <code>fsid</code> ，map创建时间和修改时间，存储池的列表，副本大小，PG个数，OSDs的列表以及他们的状态（例如 <code>up</code> ， <code>in</code> ）。查看OSD map，执行： <code>ceph osd dump</code> 。</p>
<p>3.The PG Map: 包括PG的版本，它的时间戳，最近的osd map 版本，每个pg的full ratios和细节，比如PG ID，Up Set，PG的状态（例如 <code>active+clean</code>），以及每个存储池的数据空间使用率。查看：<code>ceph pg dump</code></p>
<p>4.The CRUSH Map: 包含存储设备，故障域层级（例如，设备，host，rack，row，room等），以及存储数据时遍历层级的规则。查看一个CRUSH的map，执行<code>ceph osd getcrushmap -o {filename}</code> ；然后通过执行 <code>crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename}</code> 反编译。可以在文本编辑器或者 cat 命令查看反编译map。</p>
<p>5.The MDS Map: 包含当前MDS映射周期，map的创建时间，最后的修改时间。 同时包含了存储员数据的池子，元数据服务器的列表，以及元数据服务器状态列表信息。 查看MDS map，执行 <code>ceph mds dump</code> 。</p>
<p>每一个map包含了一个操作状态修改的迭代历史。Ceph Monitor维护了一个集群map的主副本信息，包括集群成员，状态，修改，以及整个存储集群的健康状况。</p>
<h3 id="高可用Monitors"><a href="#高可用Monitors" class="headerlink" title="高可用Monitors"></a>高可用Monitors</h3><p>在客户端读写数据前，需要先联系Monitor来获得集群map的最近主副本。Ceph存储集群在只有一个Mon下也能够正常工作，不过这样会导致单点故障（例如，如果monitor宕机，客户端就不能读写数据了）</p>
<p>为了提升可用和容错性能，Ceph支持monitors集群。在monitors集群中，延迟和其他错误会导致一个或者更多的monitors的状态落后于当前集群状态。因此，Ceph必须在大量monitor实例中达成一致，不管集群的状态。Ceph经常使用monitor大多数原则（例如，1,2:3,4:5,4:6，等）和<a href="https://en.wikipedia.org/wiki/Paxos_(computer_science" target="_blank" rel="external">Paxos</a>)算法来选举一致认同的集群环境状态版本。</p>
<p>monitors配置的详细说明见 <a href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref" target="_blank" rel="external">Monitor Config Reference</a></p>
<h3 id="高可靠性验证"><a href="#高可靠性验证" class="headerlink" title="高可靠性验证"></a>高可靠性验证</h3><p>为了区分用户以及抵御hacker攻击，Ceph提供 Cephx 验证系统用于授权用户和守护进程。</p>
<blockquote>
<p>注意，Cephx协议并不在传输过程或者其他对数据加密（例如，SSL/TLS）。</p>
</blockquote>
<p>Ceph使用共享密钥来授权，意味着客户端和monitor集群都会有客户端的密钥。认证协议确保了双方能够通过使用密钥副本实现交互连接而不用暴露自己拥有的密钥。这就实现了彼此的认证，意味着集群确认了客户拥有密钥，同时客户端确认集群拥有密钥的副本。</p>
<p>Ceph密钥的可扩展特性表现在于，它避免了集中式的接口认证，Ceph客户端能够直接与OSDs连接。为了保护数据的安全合法，Ceph实现了cephx的认证系统，认证的用户可启动Ceph集群客户端运行模式。cephx协议操作方式类似与Kerberos（一种网络认证协议）。</p>
<p>用户或者执行者调用Ceph客户端来和monitor通信。不同于Kerberos，每一个monitor都可以授权用户和分发密钥，所以不存在单点故障或者瓶颈。monitor返回一个类似于Kerberos ticket的授权数据结构，包含一个回话钥匙用来获取Ceph服务。这个会话钥匙本身通过用户的永久密钥加密，所以只有对应的用户才能向Ceph monitor(s)请求服务。客户端然后使用会话钥匙来向monitor请求服务，monitor会提供ticket给客户端，授权客户端访问OSDs处理数据的权限。Ceph monitors和OSDs共享一个密钥，因此客户端可以使用monitor提供的ticket访问集群中的任意OSD或者元数据服务器。与Kerberos相同的是， cephx 的tickets会过期，所以攻击者不能偷偷的使用过期的ticket或者会话钥匙。这种形式的授权可以防止攻击者通过访问通信媒介用其他用户的id伪造假消息或者改变其他用户正常的消息，只要用户的密钥不在过期前泄露。</p>
<p>要使用 <code>cephx</code> ，管理员必须先设定用户。如下图示， <code>client.admin</code>用户在命令行执行 <code>ceph auth get-or-create-key</code> 来生成用户名和密钥。Ceph的 auth 子系统生成用户名和密钥，存储一份副本存储在monitor(s)并传递回 <code>client.admin</code> 用户。这意味着客户端和monitor共享一个密钥。</p>
<blockquote>
<p>注意， <code>client.admin</code> 必须用安全的方法把用户ID和密钥发送给用户。</p>
</blockquote>
<p><img src="http://i.imgur.com/3ZCiwLl.png" alt=""></p>
<p>为实现与monitor认证，客户端需发送用户名给monitor，monitor生成会话钥匙并使用用户名对应的密钥加密。然后，monitor发送加密后的ticket给client。客户端然后用共享密钥解密传输数据来获得会话密钥。会话密钥标识了当前会话的用户。客户端然后请求一个用会话密钥签名代表用户的ticket。monitor生成ticket，用用户的密钥加密然后回传给客户端。客户端解密ticket然后使用它签名访问集群的OSDs和元数据服务器。</p>
<p><img src="http://i.imgur.com/95Y1ToP.png" alt=""></p>
<p>cephx 协议认证能实现客户端设备与Ceph服务的持续通讯。每一条传递在在客户端和服务器之间的信息，初始化认证后，可通过使用ticket充当认证Mon、OSDs、MDS的共享密钥。</p>
<p><img src="http://i.imgur.com/m1DgSjd.png" alt=""></p>
<p>授权提供的Ceph客户端和服务器主机之间的协议。授权不会超出Ceph客户端之外。如果用户从远程主机访问Ceph客户端，Ceph的授权不会应用到用户主机到客户端主机之间。</p>
<p>详细的配置，见<a href="http://ceph.com/docs/master/rados/configuration/auth-config-ref" target="_blank" rel="external">Ceph Config Guide</a>。用户管理的详细配置，见<a href="http://ceph.com/docs/master/rados/operations/user-management" target="_blank" rel="external">User Mangement</a>。</p>
<h3 id="支撑超大规模的智能守护进程"><a href="#支撑超大规模的智能守护进程" class="headerlink" title="支撑超大规模的智能守护进程"></a>支撑超大规模的智能守护进程</h3><p>在许多集群架构中，集群成员的主要目的是提供中心化的接口，就是节点可以访问。这类中心化的接口通过双重调度向客户端提供服务–这就会在PB到EB规模时造成了巨大的瓶颈。</p>
<p>Ceph消除了上述的瓶颈：实现了OSDs守护与客户端交互的智能化。类似于Ceph客户端，每一个OSD守护进程知道集群中其他OSD的状况。这使得OSD守护进程可以和其他OSD守护进程和monitors直接通信。除此以外，ceph客户端能够直接和OSD守护进程交互。</p>
<p>Ceph客户端，monitor，OSD守护进程能够相互交互，意味着OSD守护进程可以使用任意节点的CPU和RAM内存资源完成甚至是超额的任务（如：数据I/O操作）。这种计算能力的利用会带来如下几点益处：</p>
<p>1.OSDs服务直连客户端：任何的网络设备都有支持并发连接数量的限制，一个集中化的系统在可扩展性方面有着更小的物理限制。通过使客户端和OSD守护进程直连，Ceph同时增加了性能和整个系统的容量，并移除了单点故障。当需要时，客户端可以和OSD守护进程保持一个会话，不需要申请获取中心服务器。</p>
<p>2.OSD成员和状态：OSD守护进程加入集群中并汇报自身的运行状态。在最低状态等级，OSD守护进程状态有up或down两种：服务是否运行或已经关闭。如果OSD守护进程状态是 down 和 in ，这个状态表示OSD守护进程运行失败。如果OSD守护进程不在运行（例如宕机了），OSD不能通知Monitor它 down 了。monitor可以周期性的ping OSD守护进程来确定他是运行状态。然而，Ceph同样授权OSD能够尝试去判定（OSDs间心跳检测）相邻的OSD运行状态是否为down，从而更新crush map以及汇报最新状态到monitor(s)。这意味着monitors可以保持为轻量级的处理方法。额外的细节见Monitoring OSDs和Heartbeats。</p>
<p>3.数据scrub：为了维护数据的一致性和合法，OSD守护进程可以在pg内scrub对象。即：OSD守护进程比较一个PG内的对象和分别在其他OSD上的各个副本的对象元数据比较。scrub方法（默认一天一次）捕获bugs或者文件系统错误。OSD守护进程同样通过逐位比较对象中数据执行deep scrub。deep scrub（默认一周一次）会发现轻量scrub不能发现的硬盘坏扇区。清洗的scrub配置见Data Scrubbing</p>
<p>4.副本：和客户端意义，OSD守护进程使用CRUSH算法，不同的是，OSD使用算法来计算对象的副本数据存储位置，同时实现数据的均衡。在典型的写场景，客户端使用CRUSH算法来计算存放对象的位置，映射对象到pool和pg，然后查看CRUSH映射来发现pg的主OSD。</p>
<p>客户端把对象写到对应pg的主OSD。然后，主OSD在自己的CRUSH map中找到第二和第三OSDs来放置副本，然后复制对象到第二第三OSDs（额外副本数）中对应的pg，在确认数据成功存储后回应给客户端。</p>
<p><img src="http://i.imgur.com/NN1QUZa.png" alt=""></p>
<p>OSD守护进程实现数据副本存储方式，减轻了客户端的操作任务，同时确保了数据高可用性和数据安全。</p>
<h2 id="集群动态管理"><a href="#集群动态管理" class="headerlink" title="集群动态管理"></a>集群动态管理</h2><p>在扩展性和高可用性章节，解释了Ceph如何使用CRUSH，集群感知和智能的守护进程保证了扩展性和高可用性。Ceph设计的关键是自动化，自修复，以及智能的OSD守护进程。我们来更深层次的了解CRUSH如何使得云存储基础架构方案真正的实现数据存储，动态智能的均衡集群数据负载和修复错误。</p>
<h3 id="关于Pools"><a href="#关于Pools" class="headerlink" title="关于Pools"></a>关于Pools</h3><p>Ceph的存储系统支持 Pools 的概念，表示集群实现存储object的逻辑分区。</p>
<p>Ceph客户端从monitor检索Cluster Map，并写对象到pools。pool的大小或者副本的个数，CRUSH规则集以及pg的数量会决定Ceph如何放置数据。</p>
<p><img src="http://i.imgur.com/y18VDFp.png" alt=""></p>
<p>Pools至少设置需要如下参数：</p>
<ul>
<li><p>对象的所有权或者访问权</p>
</li>
<li><p>PG的数量</p>
</li>
<li><p>要使用的CRUSH规则集</p>
</li>
<li></li>
</ul>
<p>详细见<a href="http://docs.ceph.com/docs/master/rados/operations/pools/#set-pool-values" target="_blank" rel="external">Set Pool Values</a></p>
<h3 id="映射：PGs-gt-OSDs"><a href="#映射：PGs-gt-OSDs" class="headerlink" title="映射：PGs-&gt; OSDs"></a>映射：PGs-&gt; OSDs</h3><p>每个pool有一定数量的pg。CRUSH直接映射PGs到OSDs。当客户端存储数据时，CRUSH映射每一个对象到pg。</p>
<p>映射对象到pg在OSD守护进程和客户端之间创建了一个间接层。Ceph存储集群必须能够在存数据时动态的扩大（或者收缩）以及动态平衡。如果客户端知道某个OSD守护进程所拥有的对象，这就在客户端和OSD守护进程创建了一个紧密的耦合。相反，CRUSH算法映射每一个对象到pg然后映射每一个pg到一个或者多个OSD守护进程。这一层费直接联接允许Ceph在新OSD守护进程上线或者失效OSD守护进程恢复时动态平衡。下面的图描述了CRUSH如何映射对象到pgs，以及pgs到OSDs。</p>
<p><img src="http://i.imgur.com/Z1V046i.png" alt=""></p>
<p>利用集群映射和CRUSH算法，客户端可以在读写数据时精确计算出需要使用的OSD。</p>
<h3 id="计算PG的ID"><a href="#计算PG的ID" class="headerlink" title="计算PG的ID"></a>计算PG的ID</h3><p>当一个客户端绑定到monitor，他会检索集群映射的最新副本。有了集群映射后，客户端就知道了集群中所有的monitors，OSDs，和元数据服务器。然而，它不知道任何关于对象位置的信息。</p>
<blockquote>
<p>对象的位置是通过计算得到的。</p>
</blockquote>
<p>客户端唯一需要的输入是对象ID和pool。很简单：Ceph存储数据在有命名的pool中（例如，”liverpool”）。当客户端想要存储有命名的对象（例如: “john”, “<br>paul”, “george”, “ringo” 等），它会用对象名的哈希值，pool中的PGs数，pool名字计算pg。客户端使用如下步骤计算：</p>
<ol>
<li>客户端输入pool ID和对象ID（例如，pool=”liverpool”, object-id=”john”）。</li>
<li>Ceph拿到对象ID然后计算哈希值。</li>
<li>用哈希值取余PGs的数量（例如，58）然后得到PG ID。</li>
<li>Ceph用pool获得pool ID（例如，”liverpool”=4）。</li>
<li>Ceph连接pool ID到PG ID前面（例如，4.58）</li>
</ol>
<p>计算对象位置远快于在非正式会话中查询对象位置。CRUSH算法允许客户端来计算对象要存的位置，并且允许客户端连接主OSD来存储或者检索对象。</p>
<h3 id="Peering-and-sets"><a href="#Peering-and-sets" class="headerlink" title="Peering and sets"></a>Peering and sets</h3><p>在前面的章节中，我们注意到OSD守护进程互相检查心跳然后报告给monitor。OSD守护进程做的另外一个是 ‘peering’ ，该方法使所有存储Placement Group(PG)的OSDs对PG中所有对象（以及他们的元数据）的状态达成一致。事实上，OSD守护进程报告对等失败给monitors。同步问题通常是自己解决；然而，如果这个问题仍然存在，可以参考<a href="http://docs.ceph.com/docs/master/rados/troubleshooting/troubleshooting-pg/#placement-group-down-peering-failure" target="_blank" rel="external">Troubleshooting Peering Failure</a>章节。</p>
<blockquote>
<p>注意：状态达成一致并不意味着PGs有最新的内容。</p>
</blockquote>
<p>存储集群被设计为至少存储对象最新的两份副本（也就是size=2），这是数据安全的最低需求。为了高可用性，存储集群应该存储更多的副本（例如， size=3 和 min size=2），所以当集群处于维护数据安全的degraded（降级）状态时任能够运行。</p>
<p>看支撑超大规模的智能守护进程中的图表章节，我们并不具体的命名每个OSD守护进程（例如，osd.0,osd.1等)，而更多的是成做主的，第二的，以及第四的。按照惯例，主要的是活动集中的第一个，负责在作为主要的地方为每个pg协调对等进程，以及作为唯一OSD接收客户端初始的写对象到主要placememt group的请求。</p>
<p>当一系列的OSDs负责一个pg时，我们把他们称为活动集(Acting Set)。一个活动集会涉及到当前负责pg的OSD守护进程，或者在一些周期负责特定pg的OSD守护进程。</p>
<p>活动集中部分的OSD守护进程可能并不总是 up 。当活动集中某个OSD处于 up ，他就属于 up 集。 up 集一个重要的区分，因为当OSD失效时，Ceph可以重新映射PGs到其他OSD守护进程。</p>
<blockquote>
<p>注意：PG的活动集中包含 osd.25, osd.32, osd.61，第一个OSD， osd.25 就是主的。如果哪个OSD失效了，第二的OSD， osd.32 ，就变成了主的， osd.25 就会从Up集中移除。</p>
</blockquote>
<h3 id="重新均衡"><a href="#重新均衡" class="headerlink" title="重新均衡"></a>重新均衡</h3><p>当添加一个OSD守护进程到存储集群时，集群映射就会用新的OSD更新。回看计算PG IDs章节，会改变集群映射。因此，它改变了对象放置，因为它改变了计算的输入。下面的图描述了平衡过程（尽管非常粗糙，因为它在大集群中基本影响很小），一些但并不是所有PGs从已有的OSDs（OSD 1和OSD 2）迁移到新的OSD（OSD 3）。就算在重新平衡中，Ceph也是稳定的。许多pg仍然是原来配置，每一个OSD获得一些新的容量，所以在重新平衡结束后新OSD不会出现负载峰值。</p>
<p><img src="http://i.imgur.com/ZqVIh3I.png" alt=""></p>
<h3 id="数据一致性"><a href="#数据一致性" class="headerlink" title="数据一致性"></a>数据一致性</h3><p>作为维护数据一致性和干净的一部分，Ceph同样可以在pg内scrub对象。也就是说，OSDs可以在一个pg内和其他OSDs的pg存储的副本比较对象元数据。scrub（通常是一天一次）捕获OSD的bugs和文件系统错误。OSD守护进程同样通过逐位比较对象中数据执行deep scrub。deep scrub（通常一周一次）会发现scrub不能发现的硬盘坏扇区。</p>
<p>清洗的详细配置见<a href="http://docs.ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing" target="_blank" rel="external">Data Scrubbing</a>。</p>
<h2 id="纠删码"><a href="#纠删码" class="headerlink" title="纠删码"></a>纠删码</h2><p>使用纠删码的pool存储每个对象为 K+M 块。将每个对象分为K个数据块和M个编码块。该pool被配置为K+M大小，所以每个数据块都存在单独的一个OSD。数据块的存储序列按照数据object的属性进行排列。。</p>
<p>例如，使用纠删码的pool被创建有5个OSDs（K+M=5），支持两块丢失（M=2）。</p>
<h3 id="读写编码数据块"><a href="#读写编码数据块" class="headerlink" title="读写编码数据块"></a>读写编码数据块</h3><p>当包含 ABCDEFGHI 的对象 NYAN 写到pool时，纠删编码功能将内容数据简单的分割到3个数据块：第一块包含 ABC ，第二块包含 DEF 和最后一块 GHI 。如果内容不是K的倍数，会添加内容直到满足。该函数同时创建了两个编码块：第四块 YXY 和第五块 GQC 。每一块都存储到一个的OSD中。这些数据块会存到名字（ NYAN）相同的对象中，但是在不同的OSDs上。除了名字，数据块的创建顺序也必须被保留，并存储在对象的属性中（ shard_t )。块1 包含 ABC ，存储在OSD5上，而块4包含 YXY 存储在OSD3上。</p>
<p><img src="http://i.imgur.com/iqPVUcU.png" alt=""></p>
<p>当对象NYAN从纠删编码的pool中读数据时，解码函数读取三个块，块1 包含 ABC ，块3包含 GHI ，块4包含 YXY 。然后它重建了原始对象 ABCDEFGHI 。解码函数得知块2和块5缺失（擦除）。块5不能读取是因为OSD4失效了。在三块读取后，解码函数就可以被调用了：OSD2最慢的并且它的数据块不算数。</p>
<p><img src="http://i.imgur.com/rELGQFi.png" alt=""></p>
<h3 id="INTERRUPTED-FULL-WRITES"><a href="#INTERRUPTED-FULL-WRITES" class="headerlink" title="INTERRUPTED FULL WRITES"></a>INTERRUPTED FULL WRITES</h3><p>在纠删编码pool中，up集中的主OSD接收所有的写操作。它负责K+M数据块的数据编码，然后发送到其他OSDs。它同时负责维护pg日志的权威版本。</p>
<p>在下面的图中，一个纠删码pg被创建为 K=2，M=1 共3个OSDs，2个放K，1个放M。pg的活动集由 OSD 1， OSD 2 和 OSD 3 组成。一个对象被编码存储到OSDs上：块D1v1（数据块 1，版本 1）在 OSD 1 上，D2v1在 OSD 2 上，C1v1（编码块 1，版本 1）在 OSD 3 上。每一个OSD的pg的日志是完全一样的(1,1 代表时间戳1，版本1)。</p>
<p><img src="http://i.imgur.com/JSahM49.png" alt=""></p>
<p>OSD 1是主OSD，负责接收客户端的完全写入（WEITE FULL），意味着要实现将原来的数据全部替换而不是重写其中的一部分。对象的第二版本（v2）是用来覆盖版本1（v1）。OSD 1把有效数据编码为3块：D1v2（数据块1，版本2）会在OSD1，D2v2在OSD2，C1v2（编码块1版本2）在OSD3.每一块会发送到目标OSD，包括用来存储块并且处理些操作以及维护placementgroup权威日志的主要OSD。当OSD收到写数据块消息时，它同时创建了pg的新记录来表示这个修改。例如，OSD 3存好C1v2后，就添加了记录 1,2（时间戳1，版本2）到日志。因为OSDs是异步工作的，所以会出现一些块还没有写入（例如D2v2)，而其他的已经确认写入磁盘了（例如C1v1和D1v1)</p>
<p><img src="http://i.imgur.com/VJ4pAji.png" alt=""></p>
<p>如果整个集群运行状态良好，那么数据块能实现存储在活动集的各个OSD上，并且日志中的 last_complete 指针会从 1,1 移到 1,2。</p>
<p><img src="http://i.imgur.com/tsjzfx5.png" alt=""></p>
<p>如下图所示（一般情况），最后，用于存储object数据块的旧版本数据或文件将会被删除，删除后：OSD1上的D1v1，OSD2上的C1v1，OSD3上的C1v1。</p>
<p><img src="http://i.imgur.com/nXpimbX.png" alt=""></p>
<p>特殊情况。OSD1处于down状态，而同时D2v2一直在迁移，，对象的版本2只写了一部分：OSD3有1个块，但是利用一个数据块不足以实现数据的编码恢复功能。它就会丢失两个块：D1v2和D2v2，纠删码参数K=2,M=1，意味着需要至少2个块来重建第三块。OSD4成为了新的主OSD，发现日志记录 last_complete （也就是，这条记录之前的所有对象是可以在之前的活动集的OSDs获得的）是1,1，然后这会变成新的权威日志的head。</p>
<p><img src="http://i.imgur.com/lUIZqYg.png" alt=""></p>
<p>OSD3上发现的日志记录 1,2 和OSD4上新的权威日志有分歧：然后他就被抛弃了，包含块C1v2块的文件就被删除了。然后D1v1块会在数据清理时通过解码功能重建并存储新的主OSD 4。</p>
<p><img src="http://i.imgur.com/4sDFCUE.png" alt=""></p>
<p>额外的细节见<a href="https://github.com/ceph/ceph/blob/40059e12af88267d0da67d8fd8d9cd81244d8f93/doc/dev/osd_internals/erasure_coding/developer_notes.rst" target="_blank" rel="external">Erasure Code Notes</a></p>
<h2 id="分层缓存"><a href="#分层缓存" class="headerlink" title="分层缓存"></a>分层缓存</h2><p>分层可以给客户端提供后端存储层的数据子集更好的I/O表现。缓存层包含相对快速，昂贵存储设备（例如，ssd）上创建的pool，配置用来当做缓存层，以及一个后端pool，不管是纠删编码的或者是相对缓慢、便宜的设备配置为商业存储层。Ceph对象处理放置对象的位置，层级代理决定向缓存还是后端存储写入对象。所以，缓存层和后端存储层对客户端来说都是透明的。</p>
<p><img src="http://i.imgur.com/df9JTKb.png" alt=""></p>
<p>额外细节见<a href="https://github.com/ceph/ceph/blob/40059e12af88267d0da67d8fd8d9cd81244d8f93/doc/dev/osd_internals/erasure_coding/developer_notes.rst" target="_blank" rel="external">Cache Tiering</a></p>
<h2 id="扩展Ceph"><a href="#扩展Ceph" class="headerlink" title="扩展Ceph"></a>扩展Ceph</h2><p>可以通过创建共享对象类 Ceph class 来扩展Ceph。Ceph加载动态存储在osd class dir 目录（如：默认目录$libdir/rados-classes）下的.so类库。当你实现一个类后，就能够创建一个新的有能够在Ceph对象存储器调用本地方法的object方法，或通过利用函数库创建其它混合的类方法或全都有自己实现的构造方法。</p>
<p>在写的时候，Ceph类可以调用本地或者类方法，可以对未归纳数据（inbound data）执行任何系列操作，然后生成Ceph的原子级应用的结果写入事务。</p>
<p>在读的时候，Ceph类可以调用本地或者类方法，可以对出站数据（outbound data）执行系列操作然后返回数据到客户端。</p>
<blockquote>
<p><strong>Ceph类样例</strong></p>
<p>内容管理系统的呈现特定大小和纵横比图片的Ceph类，可以读取归类的位图图像，裁剪到特定纵横比，改变大小，并嵌入不可见的版权或者水印俩帮助保护知识版权；然后保存结果位图到对象存储。</p>
</blockquote>
<p>可以通过 <code>src/objclass/objclass.h</code>， <code>src/fooclass.cc</code> 和 <code>src/barclass</code> 查看样例实现。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Ceph存储集群就像充满活力的有机体。然而，许多存储应用并没有完全应用典型CPU和RAM，但是Ceph做到了。从心跳，到对等，到平衡集群或者从故障中恢复，Ceph减轻了客户端的责任（以及解除了ceph架构中不需要的中心化的网关），并使用了OSDs的计算能力执行任务。参考<a href="http://docs.ceph.com/docs/master/start/hardware-recommendations/" target="_blank" rel="external">Hardware Recommendations</a>和<a href="http://docs.ceph.com/docs/master/rados/configuration/network-config-ref/" target="_blank" rel="external">Network Config Reference</a>来理解Ceph如何利用计算资源的概念。</p>
<h1 id="Ceph-协议"><a href="#Ceph-协议" class="headerlink" title="Ceph 协议"></a>Ceph 协议</h1><p>Ceph客户端使用本地协议来和存储集群交互。Ceph把这些功能打包到了 librados 库，所以可以创建你自己的客户端。下面的图描述了基本的架构。</p>
<p><img src="http://i.imgur.com/Znlg4s0.png" alt=""></p>
<h2 id="本地协议和-LIBRADOS"><a href="#本地协议和-LIBRADOS" class="headerlink" title="本地协议和 LIBRADOS"></a>本地协议和 LIBRADOS</h2><p>现代应用需要一个有异步通信能力的简单对象存储接口。Ceph存储集群就提供了一个简单的对象存储接口用以实现异步通讯功能。接口提供了在整个集群中直接，并行访问对象的能力。</p>
<ul>
<li>Pool操作</li>
<li>快照和写入时复制的克隆</li>
<li>读写对象-创建或删除-整个对象或比特范围-追加或截断</li>
<li>创建/设置/获取/删除 扩展属性(XATTRs)</li>
<li>创建/设置/获取/删除 键值对</li>
<li>混合操作或双重响应语义</li>
<li>object类</li>
</ul>
<h2 id="对象观察-通知"><a href="#对象观察-通知" class="headerlink" title="对象观察/通知"></a>对象观察/通知</h2><p>客户端可以对一个对象注册持久关注并保持和主OSD会话的打开。客户端可以发送通知消息并通过有效数据传输给所有观察者，在观察者收到通知后会收到回传通知。这使得客户端可以使用任何对象同步/通信通道。</p>
<p><img src="http://i.imgur.com/3iCMLYi.png" alt=""></p>
<h2 id="数据条带化"><a href="#数据条带化" class="headerlink" title="数据条带化"></a>数据条带化</h2><p>存储设备有吞吐量的限制，从而影响性能和可伸缩性。因此，存储系统在跨多个存储设备往往支持连续块条带化存储信息，以提高吞吐量和性能。数据条带化最常见的形式是RAID。和Ceph的条带化最类似的是RAID 0，或者 ‘striped volume’ 。Ceph的条带化提供了RAID 0条带，N路可靠的RAID镜像和更快的恢复性。</p>
<p>Ceph提供三类客户端：Ceph块设备，Ceph文件系统，Ceph对象存储。Ceph客户端把数据从它提供给用户的描述模式（一个块设备镜像，基于RESTful对象，CephFS文件系统目录）转换为用来存储到Ceph存储系统的对象。</p>
<blockquote>
<p>提示： Ceph存储系统存储的对象并没有条带化的。Ceph对象存储，Ceph块设备和Ceph文件系统在存储集群上多个对象上条带化数据。客户端通过 librados 直接写到存储集群时必须进行条带化（和并行I/O）来获得这些益处。</p>
</blockquote>
<p>Ceph条带化最简单的格式是条带数为1的对象。Ceph的客户端写入条带单元的Ceph的存储集群对象，直到对象是其最大容量，然后创建新的对象在存储额外的条带数据。最简单的条带化形式可能是足够多的小块设备，S3、Swift对象和CephFS文件。然而，这种最简单的形式没有最大化的利用Ceph跨pg分发数据的能力，因此并没有很好的改进性能。下面的图描述了条带化的最简单的形式。</p>
<p><img src="http://i.imgur.com/Jl6ePPg.png" alt=""></p>
<p>如果需要使用大的镜像，大的S3或者Swift对象（例如视频），或者大的CephFS目录，你可以看到通过在多个对象的条带化客户端数据带来的客观的读写性能改进。当客户端并行写条带单元到对象会得到客观的写性能。犹豫对象映射到不同的pg以及进一步映射到不同的OSDs，每一次写都以最大的写速度并行发生。单个磁盘的写会被磁头的移动（例如，每次6ms的寻道）和单个设备的带宽（例如，100MB/s)限制。通过在多个对象间（映射到了不同的pg和OSDs）传播写操作，Ceph可以减少每个硬盘的寻道次数并合并多个硬盘来获得更快的写（或读）速度。</p>
<blockquote>
<p>注意：条带化的是独立的对象副本。CRUSH通过OSDs实现副本，条带化会自动复制副本。</p>
</blockquote>
<p>在下面的图中，客户端数据得到一个包含4个对象的对象集（图中的 object set 1 ），其中第一个条带单元是 strip unit 0 位于 object 0 ，第4块条带单元 stripe unit 3 位于 object 3 。在写完这4个条带后，客户端决定对象集是否满了。如果对象集没有满，客户端开始再次写入条带到第一个对象（下图中的 object 0 )。如果对象集满了，客户端创建一个新的对象集（ 下图中的 object 2 )，然后开始在新对象集中的第一个对象（下图中的 object 4 ）写第一个条带（ stripe unit 16 ）。</p>
<p><img src="http://i.imgur.com/WwCMDZX.png" alt=""></p>
<p>3个变量决定了Ceph如何条带数据：</p>
<ul>
<li>Object Size: 存储集群中的对象有一个最大配置大小（例如，2MB，4MB等）。对象的大小要足够大能够容纳许多条带单元，而且应该是条带单元大小的倍数。</li>
<li>Stripe Width: 条带会有可配置的单元大小（例如，64kb）。客户端会把要写入对象的数据划分为同样大小的条带单元，除了最后一个条带单元。条带的宽度，需要是对象大小的分数，这样一个对象才能包含许多分段单元。</li>
<li>Stripe Count: 客户端连续的写条带单元到一系列的对象，这一系列对象个数由stripe count决定。这一系列的对象称为对象集。在客户端写完对象集的最后一个对象后，它会返回对象集中的第一个对象。</li>
</ul>
<blockquote>
<p>重要：在把集群投入生产前测试你的条带化配置性能。在条带化数据并写入对象后，就不能改变条带参数了。</p>
</blockquote>
<p>一旦客户端把有条带化单元的条带化数据并且映射条带化单元至对象后，Ceph的CRUSH算法会在对象以文件形式存储到磁盘前，把对象映射到pg，以及映射pg到OSD守护进程。</p>
<blockquote>
<p>注意：因为客户端写到单个pool中，所有的数据条带化的对象映射到同一个pool中的pg。所以他们使用相同的CRUSH映射和相同的访问控制。</p>
</blockquote>
<h1 id="Ceph-客户端"><a href="#Ceph-客户端" class="headerlink" title="Ceph 客户端"></a>Ceph 客户端</h1><p>Ceph客户端包括一系列服务接口，包括：</p>
<ul>
<li>块设备：Ceph块设备（又名RBD）服务提供可调整大小，自动精简配置的块设备，拥有快照和克隆功能。Ceph的条带块设备跨集群的高性能。Ceph支持内核对象（KO）和直接使用 librdb 的QEMU虚拟机-避免内核对象出现在虚拟系统上层。</li>
<li>对象存储：Ceph对象存储（又名，RGW）服务提供RESTful接口，兼容Amazon S3和OpenStack Swift。</li>
<li>文件系统：Ceph 文件系统（CephFS）服务提供POSIX兼容的文件系统，可以使用 mount 作为文件系统或者在用户空间（FUSE）。</li>
</ul>
<p>Ceph可以运行额外的OSDs，MDSs和Monitors实例来扩展和获得高可用性。下图描述了上层架构。</p>
<p><img src="http://i.imgur.com/ZJqRvHm.png" alt=""></p>
<h2 id="Ceph对象存储"><a href="#Ceph对象存储" class="headerlink" title="Ceph对象存储"></a>Ceph对象存储</h2><p>Ceph的对象存储守护进程， radosgw ，是一个FastCGI服务，提供RESTful HTTP API来存储对象和元数据。它有自己的数据格式，位于Ceph存储集群的顶层，并维护自己的用户数据库，授权和访问控制。RADOS网关使用统一的命名空间，这意味着，你可以使用OpenStack Swift兼容的API或者Amazon S3兼容的API。例如，你可以使用S3兼容的API写数据，然后在另一个应用用Swift兼容的API来读数据。</p>
<blockquote>
<p><strong>S3/Swift对象和存储集群对象比较</strong></p>
<p>Ceph的对象存储使用术语 对象 来描述存储的数据。S3和Swift对象和Ceph写到存储集群的对象并不一样。Ceph对象存储的对象是映射到Ceph存储集群的对象。S3和Swift对象并不需要和存储集群存储的对象以1:1的方式符合。这是可能的一个S3或Swift对象映射到多个Ceph的对象。</p>
</blockquote>
<p>详见Ceph<a href="http://docs.ceph.com/docs/master/radosgw/" target="_blank" rel="external">对象存储</a></p>
<h2 id="Ceph块设备"><a href="#Ceph块设备" class="headerlink" title="Ceph块设备"></a>Ceph块设备</h2><p>Ceph块设备在Ceph存储集群中的多个对象上对块设备映像进行条带化，其中每个对象都映射到一个pg并进行分发，并且这些pg遍布整个集群中的单独的ceph-osd守护程序。</p>
<blockquote>
<p>条带化技术可以使RBD块设备比单个服务器表现更好！</p>
</blockquote>
<p>自精简配置且可快照的块设备对虚拟化和云计算来说是吸引人的选择。在虚拟机场景，人们通常使用在QEMU/KVM中的rbd网络存储驱动部署Ceph的块设备，主机使用 librbd 来提供块设备服务给客户。许多云计算服务商使用 libvirt 来融合虚拟管理系统。相比其他解决方案，你可以使用自精简配置的Ceph块设备和Qemu， libvirt 来提供OpenStack和CloudStack服务。</p>
<p>虽然我们当前的 librbd 不支持其他的虚拟管理系统，你同样可以使用Ceph块设备内核对象来提供块给设备。其他虚拟技术例如Xen可以访问Ceph的块设备内核对象。这些可以通过命令行工具 rbd 。</p>
<h2 id="Ceph文件系统"><a href="#Ceph文件系统" class="headerlink" title="Ceph文件系统"></a>Ceph文件系统</h2><p>在基于对象的Ceph存储集群的上层，Ceph文件系统（CephFS）提供POSIX兼容的文件系统来作为服务。CephFS文件映射到Ceph存储集群存储的对象。Ceph客户端把Ceph文件系统作为内核对象或者用户空间的文件系统（FUSE）挂载。</p>
<p><img src="http://i.imgur.com/E7wYa5U.png" alt=""></p>
<p>Ceph文件系统服务包括存储集群部署的Ceph元数据服务器（MDS）。MDS的目的是存储所有的文件系统元数据（目录，文件拥有者，访问模式等）到高可靠性的元数据服务器，其上的元数据都驻留在内存。MDS（称为 ceph-mds ）存在的原因是简单文件系统操作像列出目录或者改变目录（ls，cd）没有必要加重OSD守护进程的负担。所以从数据意义中分离元数据意味着Ceph文件系统可以提供高性能服务而不用涉及存储集群。</p>
<p>CephFS从数据中分离元数据，存储元数据到MDS，并存储文件数据到存储集群的一个或多个对象。Ceph文件系统致力于POSIX兼容性。Ceph-mds可以运行为单个进程。或者他可以分发到多个物理机器，同样是为了高可用性和扩展性。</p>
<ul>
<li>高可用性：额外的 ceph-mds 实例可以作为待机，准备接管任何失效 ceph-mds 的职责。这很容易，因为所有的数据，包括日志卷，都保存在RADOS。过渡会通过 ceph-mon 自动触发。</li>
<li>扩展性：多个 ceph-mds 实例可以处于活跃状态，他们会把目录树分割为子树（并为繁忙的目录分片），有效的平衡所有活动的服务的负载。</li>
</ul>
<p>待机和活跃状态的组合是可能的，例如运行3个活跃的 ceph-mds 实例用来扩展，一个后备实例保障高可用性。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ceph/" rel="tag"># ceph</a>
          
            <a href="/tags/architecture/" rel="tag"># architecture</a>
          
        </div>
      

      
      
      

      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="uyan_frame"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="niupj" />
          <p class="site-author-name" itemprop="name">niupj</p>
           
              <p class="site-description motion-element" itemprop="description">不畏将来，不念过去</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Ceph存储集群"><span class="nav-number">1.</span> <span class="nav-text">Ceph存储集群</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#存储数据"><span class="nav-number">1.1.</span> <span class="nav-text">存储数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#可扩展和高可用性"><span class="nav-number">1.2.</span> <span class="nav-text">可扩展和高可用性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CRUSH介绍"><span class="nav-number">1.2.1.</span> <span class="nav-text">CRUSH介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集群map"><span class="nav-number">1.2.2.</span> <span class="nav-text">集群map</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#高可用Monitors"><span class="nav-number">1.2.3.</span> <span class="nav-text">高可用Monitors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#高可靠性验证"><span class="nav-number">1.2.4.</span> <span class="nav-text">高可靠性验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#支撑超大规模的智能守护进程"><span class="nav-number">1.2.5.</span> <span class="nav-text">支撑超大规模的智能守护进程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集群动态管理"><span class="nav-number">1.3.</span> <span class="nav-text">集群动态管理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#关于Pools"><span class="nav-number">1.3.1.</span> <span class="nav-text">关于Pools</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#映射：PGs-gt-OSDs"><span class="nav-number">1.3.2.</span> <span class="nav-text">映射：PGs-> OSDs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算PG的ID"><span class="nav-number">1.3.3.</span> <span class="nav-text">计算PG的ID</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Peering-and-sets"><span class="nav-number">1.3.4.</span> <span class="nav-text">Peering and sets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#重新均衡"><span class="nav-number">1.3.5.</span> <span class="nav-text">重新均衡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据一致性"><span class="nav-number">1.3.6.</span> <span class="nav-text">数据一致性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#纠删码"><span class="nav-number">1.4.</span> <span class="nav-text">纠删码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#读写编码数据块"><span class="nav-number">1.4.1.</span> <span class="nav-text">读写编码数据块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#INTERRUPTED-FULL-WRITES"><span class="nav-number">1.4.2.</span> <span class="nav-text">INTERRUPTED FULL WRITES</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分层缓存"><span class="nav-number">1.5.</span> <span class="nav-text">分层缓存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#扩展Ceph"><span class="nav-number">1.6.</span> <span class="nav-text">扩展Ceph</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">1.7.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ceph-协议"><span class="nav-number">2.</span> <span class="nav-text">Ceph 协议</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#本地协议和-LIBRADOS"><span class="nav-number">2.1.</span> <span class="nav-text">本地协议和 LIBRADOS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对象观察-通知"><span class="nav-number">2.2.</span> <span class="nav-text">对象观察/通知</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据条带化"><span class="nav-number">2.3.</span> <span class="nav-text">数据条带化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ceph-客户端"><span class="nav-number">3.</span> <span class="nav-text">Ceph 客户端</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph对象存储"><span class="nav-number">3.1.</span> <span class="nav-text">Ceph对象存储</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph块设备"><span class="nav-number">3.2.</span> <span class="nav-text">Ceph块设备</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph文件系统"><span class="nav-number">3.3.</span> <span class="nav-text">Ceph文件系统</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">niupj</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  
    

    
      <!-- UY BEGIN -->
      <script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2140076"></script>
      <!-- UY END -->
    
  





  






  





  

  

  

  

  

  

</body>
</html>
